import logging
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from src.core.query_engine.operators.base_operator import BaseOperator


class Generator(BaseOperator):
    """
    Operator for generating natural language responses using Hugging Face's Transformers.
    """

    def __init__(self, model_name="EleutherAI/gpt-neo-2.7B", device=None, seed=42):
        """
        Initialize the generator with a specified model.
        :param model_name: The Hugging Face model to use for generation.
        :param device: The device to load the model on ("cuda" for GPU or "cpu").
                       If None, the device is automatically selected.
        :param seed: Seed for reproducibility.
        """
        self.logger = logging.getLogger(self.__class__.__name__)
        self.logger.info(f"Initializing Generator with model: {model_name}...")

        # Set random seed for reproducibility
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)

        # Automatically detect device if not provided
        if device is None:
            device = "cuda" if torch.cuda.is_available() else "cpu"
        self.device = device
        self.logger.info(f"Selected device: {self.device}")

        # Load tokenizer and model
        self.logger.info(f"Loading model and tokenizer for {model_name}...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)

        # Ensure padding token is set
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        self.model = AutoModelForCausalLM.from_pretrained(
            model_name, trust_remote_code=True
        ).to(self.device)

        self.logger.info("Model and tokenizer loaded successfully.")

    def execute(self, input_data, **kwargs):
        """
        Generate a response using the model.
        :param input_data: Preformatted QA-template input string.
        :param kwargs: Additional parameters for generation.
        :return: Generated response.
        """
        try:
            # Log the received input
            self.logger.info(f"Generating response for input data:\n{input_data}")

            # Tokenize input_data
            inputs = self.tokenizer(
                input_data, return_tensors="pt", padding=True, truncation=True
            ).to(self.device)

            # Default generation parameters
            max_new_tokens = kwargs.get("max_new_tokens", 100)
            temperature = kwargs.get("temperature", None)  # Use None if not sampling
            top_p = kwargs.get("top_p", None)             # Use None if not sampling
            do_sample = temperature is not None or top_p is not None

            # Generate output
            outputs = self.model.generate(
                input_ids=inputs["input_ids"],
                attention_mask=inputs["attention_mask"],
                max_new_tokens=max_new_tokens,
                temperature=temperature if do_sample else 1.0,
                top_p=top_p if do_sample else None,
                do_sample=do_sample,
                pad_token_id=self.tokenizer.pad_token_id,
                num_return_sequences=1,  # Ensure a single output
            )

            # Decode the response
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            self.logger.info(f"Decoded output: {response}")

            # Extract the actual answer from the generated output
            answer = self._extract_answer(response)
            self.logger.info(f"Extracted answer: {answer}")

            return answer

        except Exception as e:
            self.logger.error(f"Error during response generation: {str(e)}")
            raise RuntimeError(f"Response generation failed: {str(e)}")

    def _extract_answer(self, generated_output):
        """
        Extract the answer from the generated output.
        :param generated_output: The full text generated by the model.
        :return: The extracted answer or the full output if extraction fails.
        """
        # Extract the answer portion
        if "Answer:" in generated_output:
            after_answer = generated_output.split("Answer:")[1].strip()
            # Handle cases where additional formatting is needed
            return after_answer.split("\n")[0].strip()  # Take only the first line after "Answer:"
        return generated_output.strip()
